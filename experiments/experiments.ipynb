{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "import cv2\n",
    "import sys\n",
    "import cbm\n",
    "import cbm_cfs\n",
    "import json\n",
    "import torch\n",
    "import plots\n",
    "import utils\n",
    "import random\n",
    "import argparse\n",
    "import data_utils\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.distributed as dist\n",
    "sys.path.append(\"../guided-diffusion\")\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from guided_diffusion import dist_util, logger\n",
    "from guided_diffusion.script_util import (\n",
    "    NUM_CLASSES,\n",
    "    # model_and_diffusion_defaults,\n",
    "    create_model_and_diffusion,\n",
    "    add_dict_to_argparser,\n",
    "    args_to_dict,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "trans_to_256= transforms.Compose([\n",
    "   transforms.Resize((256, 256)),])\n",
    "trans_to_224= transforms.Compose([\n",
    "   transforms.Resize((224, 224)),])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffusion_defaults():\n",
    "    \"\"\"\n",
    "    Defaults for image and classifier training.\n",
    "    \"\"\"\n",
    "    return dict(\n",
    "        learn_sigma=True,\n",
    "        diffusion_steps=1000,\n",
    "        noise_schedule=\"linear\",\n",
    "        timestep_respacing=\"250\",\n",
    "        use_kl=False,\n",
    "        predict_xstart=False,\n",
    "        rescale_timesteps=False,\n",
    "        rescale_learned_sigmas=False,\n",
    "    )\n",
    "\n",
    "\n",
    "def classifier_defaults():\n",
    "    \"\"\"\n",
    "    Defaults for classifier models.\n",
    "    \"\"\"\n",
    "    return dict(\n",
    "        image_size=256,\n",
    "        classifier_use_fp16=False,\n",
    "        classifier_width=128,\n",
    "        classifier_depth=2,\n",
    "        classifier_attention_resolutions=\"32,16,8\",  # 16\n",
    "        classifier_use_scale_shift_norm=True,  # False\n",
    "        classifier_resblock_updown=True,  # False\n",
    "        classifier_pool=\"attention\",\n",
    "    )\n",
    "\n",
    "\n",
    "def model_and_diffusion_defaults():\n",
    "    \"\"\"\n",
    "    Defaults for image training.\n",
    "    \"\"\"\n",
    "    res = dict(\n",
    "        image_size=256,\n",
    "        num_channels=256,\n",
    "        num_res_blocks=2,\n",
    "        num_heads=4,\n",
    "        num_heads_upsample=-1,\n",
    "        num_head_channels=64,\n",
    "        attention_resolutions=\"32,16,8\",\n",
    "        channel_mult=\"\",\n",
    "        dropout=0.0,\n",
    "        class_cond=False,\n",
    "        use_checkpoint=False,\n",
    "        use_scale_shift_norm=True,\n",
    "        resblock_updown=True,\n",
    "        use_fp16=True,\n",
    "        use_new_attention_order=False,\n",
    "    )\n",
    "    res.update(diffusion_defaults())\n",
    "    return res\n",
    "\n",
    "\n",
    "def create_argparser():\n",
    "    defaults = dict(\n",
    "        clip_denoised=True,\n",
    "        num_samples=1,\n",
    "        batch_size=4,\n",
    "        use_ddim=False,\n",
    "        model_path=\"./guided_diffusion/models/256x256_diffusion_uncond.pt\",\n",
    "    )\n",
    "    defaults.update(model_and_diffusion_defaults())\n",
    "    parser = argparse.ArgumentParser()\n",
    "    add_dict_to_argparser(parser, defaults)\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(clip_denoised=True, num_samples=1, batch_size=4, use_ddim=False, model_path='./guided_diffusion/models/256x256_diffusion_uncond.pt', image_size=256, num_channels=256, num_res_blocks=2, num_heads=4, num_heads_upsample=-1, num_head_channels=64, attention_resolutions='32,16,8', channel_mult='', dropout=0.0, class_cond=False, use_checkpoint=False, use_scale_shift_norm=True, resblock_updown=True, use_fp16=True, use_new_attention_order=False, learn_sigma=True, diffusion_steps=1000, noise_schedule='linear', timestep_respacing='250', use_kl=False, predict_xstart=False, rescale_timesteps=False, rescale_learned_sigmas=False)\n",
      "Logging to /tmp/openai-2024-03-20-13-32-34-566535\n",
      "creating model and diffusion...\n",
      "End of model creation\n"
     ]
    }
   ],
   "source": [
    "args = create_argparser().parse_args([])\n",
    "print(args)\n",
    "dist_util.setup_dist()\n",
    "logger.configure()\n",
    "\n",
    "logger.log(\"creating model and diffusion...\")\n",
    "d_model, diffusion = create_model_and_diffusion(\n",
    "    **args_to_dict(args, model_and_diffusion_defaults().keys())\n",
    ")\n",
    "# d_model.load_state_dict(\n",
    "#     dist_util.load_state_dict(args.model_path, map_location=\"cpu\")\n",
    "# )\n",
    "d_model.to(dist_util.dev())\n",
    "if args.use_fp16:\n",
    "    d_model.convert_to_fp16()\n",
    "d_model.eval()\n",
    "device = next(d_model.parameters()).device\n",
    "\n",
    "shape = (1, 3, 256, 256)\n",
    "steps=1000\n",
    "start=0.0001\n",
    "end=0.02\n",
    "print('End of model creation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def range_of_delta(beta_s, beta_e, steps):\n",
    "    def delta_value(beta):\n",
    "        return (beta/(1-beta))**(0.5)\n",
    "    return (delta_value(beta_s), delta_value(beta_e))\n",
    "\n",
    "def beta(t, steps, start, end):\n",
    "    return (t-1)/(steps-1)*(end-start)+start\n",
    "\n",
    "def add_noise(x, delta, opt_t, steps, start, end):\n",
    "    return np.sqrt(1-beta(opt_t, steps, start, end))*(x + th.randn_like(x) * delta)\n",
    "\n",
    "def get_opt_t(delta, start, end, steps):\n",
    "    return np.clip(int(np.around(1+(steps-1)/(end-start)*(1-1/(1+delta**2)-start))), 0, steps)\n",
    "\n",
    "\n",
    "def denoise(img, opt_t, steps, start, end, delta, direct_pred=False):\n",
    "    img_xt = add_noise(img, delta, opt_t, steps, start, end).unsqueeze(0).to(device)\n",
    "\n",
    "    indices = list(range(opt_t))[::-1]\n",
    "    from tqdm.auto import tqdm\n",
    "#     indices = tqdm(indices)\n",
    "    img_iter = img_xt\n",
    "    for i in indices:\n",
    "        t = th.tensor([i]*shape[0], device=device)\n",
    "        # t = t.to(device)\n",
    "        with th.no_grad():\n",
    "            out = diffusion.p_sample(\n",
    "                d_model,\n",
    "                img_iter,\n",
    "                t,\n",
    "                clip_denoised=args.clip_denoised,\n",
    "                denoised_fn=None,\n",
    "                cond_fn=None,\n",
    "                model_kwargs={},\n",
    "            )\n",
    "            img_iter = out['sample']\n",
    "            if direct_pred:\n",
    "                return out['pred_xstart']\n",
    "    # img_iter = ((img_iter + 1) * 127.5).clamp(0, 255).to(th.uint8)\n",
    "    # img_iter = img_iter.permute(0, 2, 3, 1)\n",
    "    # img_iter = img_iter.contiguous()\n",
    "    return img_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert VCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clip_name': 'ViT-B/16', 'backbone': 'vit', 'device': 'cuda', 'batch_size': '16', 'saga_batch_size': '256', 'dataset': 'covid', 'concept_set': 'data/concept_sets/covid_filtered_new.txt', 'feature_layer': 'norm', 'activation_dir': 'saved_activations', 'save_dir': 'saved_models', 'clip_cutoff': '0.1', 'proj_steps': '20000', 'interpretability_cutoff': '0.3', 'lam': '0.0007', 'n_iters': '10000'}\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "# change this to the correct model dir, everything else should be taken care of\n",
    "load_dir = \"./saved_models/covid_8193_21\"\n",
    "device = \"cuda\"\n",
    "num_classes = 2\n",
    "with open(os.path.join(load_dir, \"args.txt\"), \"r\") as f:\n",
    "    args1 = json.load(f)\n",
    "print(args1)\n",
    "dataset = args1[\"dataset\"]\n",
    "_, target_preprocess = data_utils.get_target_model(args1[\"backbone\"], device, dataset, num_classes)\n",
    "cbm_model = cbm.load_cbm(load_dir, device, dataset, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [01:08<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7726 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [01:06<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7586 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [01:06<00:00,  4.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7570 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [01:08<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7484 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [01:01<00:00,  5.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7396 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_d_probe = dataset+\"_val\"\n",
    "cls_file = data_utils.LABEL_FILES[dataset]\n",
    "\n",
    "val_data_t = data_utils.get_data(val_d_probe, preprocess=target_preprocess)\n",
    "val_pil_data = data_utils.get_data(val_d_probe)\n",
    "\n",
    "\n",
    "batch_size = 1\n",
    "correct = 0\n",
    "total = 0\n",
    "cfs = 0\n",
    "cpcs = 0\n",
    "trial_num = 5\n",
    "noise_level = 1/255\n",
    "attack = 5/255\n",
    "opt_t = get_opt_t(noise_level, start, end, steps)\n",
    "for j in range(5):\n",
    "    attack += (1/255)\n",
    "    for images, labels in tqdm(DataLoader(val_data_t, batch_size, num_workers=0, pin_memory=False)):\n",
    "        '''\n",
    "        images: [1, 3, 224, 224]\n",
    "        images.squeeze(0):[3, 224, 224]\n",
    "        images_denoise_smoothing: [3, 224, 224]\n",
    "        images_denoise_smoothing.unsqueeze(0): [1, 3, 224, 224]\n",
    "        '''\n",
    "        with torch.no_grad():\n",
    "            images += torch.randn_like(images) * attack\n",
    "            for i in range(trial_num):\n",
    "                images_denoised = trans_to_224(denoise(trans_to_256(images.squeeze(0)), opt_t, steps, start, end, noise_level)).detach().cpu()\n",
    "                images_denoise_smoothing = images_denoised + torch.randn_like(images, ) * noise_level\n",
    "                images_denoise_smoothing = torch.squeeze(images_denoise_smoothing)\n",
    "                images_denoise_smoothing = torch.clamp(images_denoise_smoothing, -1, 1)\n",
    "            outs, _ = cbm_model(images_denoise_smoothing.unsqueeze(0).to(device))\n",
    "#             outs, _ = cbm_model(images.to(device))\n",
    "            pred = torch.argmax(outs, dim=1)\n",
    "            correct += torch.sum(pred.cpu()==labels)\n",
    "            total += len(labels)\n",
    "    print(\"accuracy: {:.4f} \".format(correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clip_name': 'ViT-B/16', 'backbone': 'vit', 'device': 'cuda', 'batch_size': '16', 'saga_batch_size': '256', 'dataset': 'covid', 'concept_set': 'data/concept_sets/covid_filtered_new.txt', 'feature_layer': 'norm', 'activation_dir': 'saved_activations', 'save_dir': 'saved_models', 'clip_cutoff': '0.21', 'proj_steps': '20000', 'interpretability_cutoff': '0.15', 'lam': '7e-05', 'n_iters': '10000'}\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "# change this to the correct model dir, everything else should be taken care of\n",
    "load_dir = \"./saved_models/covid_cbm_2024_03_20_09_06\"\n",
    "device = \"cuda\"\n",
    "num_classes = 2\n",
    "with open(os.path.join(load_dir, \"args.txt\"), \"r\") as f:\n",
    "    args1 = json.load(f)\n",
    "print(args1)\n",
    "dataset = args1[\"dataset\"]\n",
    "_, target_preprocess = data_utils.get_target_model(args1[\"backbone\"], device, dataset, num_classes)\n",
    "cbm_model = cbm_cfs.load_cbm(load_dir, device, dataset, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [01:48<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023529411764705882_cfs: 0.5079\n",
      "0.023529411764705882_cpcs: 0.8565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [01:34<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.027450980392156862_cfs: 0.5515\n",
      "0.027450980392156862_cpcs: 0.8290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [01:30<00:00,  3.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03137254901960784_cfs: 0.5856\n",
      "0.03137254901960784_cpcs: 0.8062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [01:31<00:00,  3.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03529411764705882_cfs: 0.6146\n",
      "0.03529411764705882_cpcs: 0.7855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [01:33<00:00,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0392156862745098_cfs: 0.6400\n",
      "0.0392156862745098_cpcs: 0.7678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_d_probe = dataset+\"_val\"\n",
    "cls_file = data_utils.LABEL_FILES[dataset]\n",
    "\n",
    "val_data_t = data_utils.get_data(val_d_probe, preprocess=target_preprocess)\n",
    "val_pil_data = data_utils.get_data(val_d_probe)\n",
    "\n",
    "with open(cls_file, \"r\") as f:\n",
    "    classes = f.read().split(\"\\n\")\n",
    "\n",
    "with open(os.path.join(load_dir, \"concepts.txt\"), \"r\") as f:\n",
    "    concepts = f.read().split(\"\\n\")\n",
    "\n",
    "batch_size = 1\n",
    "correct = 0\n",
    "total = 0\n",
    "cfs = 0\n",
    "cpcs = 0\n",
    "trial_num = 5\n",
    "noise_level = 1/255\n",
    "attack = 5/255\n",
    "opt_t = get_opt_t(noise_level, start, end, steps)\n",
    "\n",
    "for j in range(5):\n",
    "    attack += (1 / 255)\n",
    "    for images, labels in tqdm(DataLoader(val_data_t, batch_size, num_workers=0, pin_memory=False)):\n",
    "        with torch.no_grad():\n",
    "            img_attack =images + torch.randn_like(images) * attack\n",
    "            for i in range(trial_num):\n",
    "                images_denoised = trans_to_224(denoise(trans_to_256(images.squeeze(0)), opt_t, steps, start, end, noise_level)).detach().cpu()\n",
    "                images_denoise_smoothing = images_denoised + torch.randn_like(images, ) * noise_level\n",
    "                images_denoise_smoothing = torch.squeeze(images_denoise_smoothing)\n",
    "                images_denoise_smoothing = torch.clamp(images_denoise_smoothing, -1, 1)\n",
    "                \n",
    "                img_attack_denoised = trans_to_224(denoise(trans_to_256(img_attack.squeeze(0)), opt_t, steps, start, end, noise_level)).detach().cpu()\n",
    "                img_attack_denoise_smoothing = img_attack_denoised + torch.randn_like(images, ) * noise_level\n",
    "                img_attack_denoise_smoothing = torch.squeeze( img_attack_denoise_smoothing)\n",
    "                img_attack_denoise_smoothing = torch.clamp(img_attack_denoise_smoothing, -1, 1)\n",
    "            \n",
    "            outs, _ = cbm_model(images_denoise_smoothing.unsqueeze(0).to(device))\n",
    "            outs1, _ = cbm_model(img_attack_denoise_smoothing.unsqueeze(0).to(device))\n",
    "            x = 0\n",
    "            y = 0\n",
    "            for i in range(len(outs[0])):\n",
    "                x = x + (outs[0][i] - outs1[0][i])**2\n",
    "                y = y + outs[0][i]**2\n",
    "            x = x**(1/2)\n",
    "            y = y**(1/2)\n",
    "            cfs += x / y\n",
    "\n",
    "            # cpcs\n",
    "            x1 = 0\n",
    "            y1 = 0\n",
    "            y2 = 0\n",
    "            for i in range(len(outs[0])):\n",
    "                x1 += outs[0][i] * outs1[0][i]\n",
    "                y1 += outs[0][i]**2\n",
    "                y2 += outs1[0][i]**2\n",
    "            y1 = y1**(1/2)\n",
    "            y2 = y2**(1/2)\n",
    "            cpcs += x1/(y1*y2)\n",
    "            total += len(labels)\n",
    "    print(\"{}_cfs: {:.4f}\".format(attack, cfs/total))\n",
    "    print(\"{}_cpcs: {:.4f}\".format(attack, cpcs/total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CBM",
   "language": "python",
   "name": "cbm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
