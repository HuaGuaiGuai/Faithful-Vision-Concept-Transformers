{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import random\n",
    "import utils\n",
    "import data_utils\n",
    "import similarity\n",
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "from glm_saga.elasticnet import IndexedTensorDataset, glm_saga\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--print'], dest='print', nargs=0, const=True, default=False, type=None, choices=None, required=False, help='Print all concepts being deleted in this stage', metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameter setting\n",
    "dataset = \"covid\"\n",
    "concept_set = \"data/concept_sets/covid_filtered_new.txt\"\n",
    "backbone = \"vit\"\n",
    "clip_name = \"ViT-B/16\"\n",
    "device = \"cuda\"\n",
    "batch_size = 16\n",
    "saga_batch_size=256\n",
    "proj_batch_size = 50000\n",
    "feature_layer = 'norm'\n",
    "activation_dir ='saved_activations'\n",
    "save_dir ='saved_models'\n",
    "clip_cutoff = 0.21\n",
    "proj_steps = 20000\n",
    "interpretability_cutoff = 0.15\n",
    "lam = 0.00007\n",
    "n_iters=10000\n",
    "prints = True\n",
    "parser = argparse.ArgumentParser(description='Settings for creating CBM')\n",
    "parser.add_argument(\"--print\", action='store_true', help=\"Print all concepts being deleted in this stage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  5.78it/s]\n",
      "100%|██████████| 27/27 [00:14<00:00,  1.82it/s]\n",
      "100%|██████████| 27/27 [00:13<00:00,  2.01it/s]\n",
      "100%|██████████| 21/21 [00:12<00:00,  1.72it/s]\n",
      "100%|██████████| 21/21 [00:11<00:00,  1.81it/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "if concept_set==None:\n",
    "    concept_set = \"data/concept_sets/{}_filtered.txt\".format(dataset)\n",
    "similarity_fn = similarity.cos_similarity_cubed_single\n",
    "    \n",
    "d_train = dataset + \"_train\" \n",
    "d_val = dataset + \"_val\"\n",
    "\n",
    "cls_file = data_utils.LABEL_FILES[dataset]\n",
    "with open(cls_file, \"r\") as f:\n",
    "    classes = f.read().split(\"\\n\")\n",
    "\n",
    "with open(concept_set) as f:\n",
    "    concepts = f.read().split(\"\\n\")\n",
    "\n",
    "num_classes = len(classes)\n",
    "# save activations and get save_paths\n",
    "for d_probe in [d_train, d_val]:\n",
    "    utils.save_activations(clip_name = clip_name, target_name = backbone, \n",
    "                            target_layers = [feature_layer], d_probe = d_probe,\n",
    "                            concept_set = concept_set, batch_size = batch_size, \n",
    "                            device = device, pool_mode = \"avg\", save_dir = activation_dir, dataset=dataset, num_classes=num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved_activations/covid_train_vit_norm.pt \n",
      " saved_activations/covid_train_clip_ViT-B16.pt \n",
      " saved_activations/covid_filtered_new_ViT-B16.pt\n",
      "saved_activations/covid_val_vit_norm.pt \n",
      " saved_activations/covid_val_clip_ViT-B16.pt \n",
      " saved_activations/covid_filtered_new_ViT-B16.pt\n"
     ]
    }
   ],
   "source": [
    "target_save_name, clip_save_name, text_save_name = utils.get_save_names(clip_name, backbone, \n",
    "                                        feature_layer,d_train, concept_set, \"avg\", activation_dir)\n",
    "val_target_save_name, val_clip_save_name, text_save_name =  utils.get_save_names(clip_name, backbone,\n",
    "                                        feature_layer, d_val, concept_set, \"avg\", activation_dir)\n",
    "print(target_save_name,\"\\n\", clip_save_name,\"\\n\", text_save_name)\n",
    "print(val_target_save_name,\"\\n\", val_clip_save_name,\"\\n\", text_save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    target_features = torch.load(target_save_name, map_location=\"cpu\").float()\n",
    "        \n",
    "    val_target_features = torch.load(val_target_save_name, map_location=\"cpu\").float()\n",
    "    \n",
    "    image_features = torch.load(clip_save_name, map_location=\"cpu\").float()\n",
    "    image_features /= torch.norm(image_features, dim=1, keepdim=True)\n",
    "\n",
    "    val_image_features = torch.load(val_clip_save_name, map_location=\"cpu\").float()\n",
    "    val_image_features /= torch.norm(val_image_features, dim=1, keepdim=True)\n",
    "\n",
    "    text_features = torch.load(text_save_name, map_location=\"cpu\").float()\n",
    "    text_features /= torch.norm(text_features, dim=1, keepdim=True)\n",
    "        \n",
    "    clip_features = image_features @ text_features.T\n",
    "    val_clip_features = val_image_features @ text_features.T\n",
    "\n",
    "    del image_features, text_features, val_image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting clear, CLIP top5:0.205\n",
      "Deleting hilar, CLIP top5:0.192\n",
      "Deleting infiltrate, CLIP top5:0.206\n"
     ]
    }
   ],
   "source": [
    "#filter concepts not activating highly\n",
    "highest = torch.mean(torch.topk(clip_features, dim=0, k=5)[0], dim=0)\n",
    "    \n",
    "if prints:\n",
    "    for i, concept in enumerate(concepts):\n",
    "        if highest[i]<=clip_cutoff:\n",
    "            print(\"Deleting {}, CLIP top5:{:.3f}\".format(concept, highest[i]))\n",
    "concepts = [concepts[i] for i in range(len(concepts)) if highest[i]>clip_cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save memory by recalculating\n",
    "del clip_features\n",
    "with torch.no_grad():\n",
    "    image_features = torch.load(clip_save_name, map_location=\"cpu\").float()\n",
    "    image_features /= torch.norm(image_features, dim=1, keepdim=True)\n",
    "\n",
    "    text_features = torch.load(text_save_name, map_location=\"cpu\").float()[highest>clip_cutoff]\n",
    "    text_features /= torch.norm(text_features, dim=1, keepdim=True)\n",
    "    \n",
    "    clip_features = image_features @ text_features.T\n",
    "    del image_features, text_features\n",
    "    \n",
    "val_clip_features = val_clip_features[:, highest>clip_cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "Step:0, Avg train similarity:-0.0099, Avg val similarity:0.0441\n",
      "Best step:100, Avg val similarity:0.2957\n"
     ]
    }
   ],
   "source": [
    " #learn projection layer\n",
    "proj_layer = torch.nn.Linear(in_features=target_features.shape[1], out_features=len(concepts),\n",
    "                                 bias=False).to(device)\n",
    "opt = torch.optim.Adam(proj_layer.parameters(), lr=1e-3)\n",
    "print(len(concepts))\n",
    "indices = [ind for ind in range(len(target_features))]\n",
    "    \n",
    "best_val_loss = float(\"inf\")\n",
    "best_step = 0\n",
    "best_weights = None\n",
    "proj_batch_size = min(proj_batch_size, len(target_features))\n",
    "for i in range(proj_steps):\n",
    "    batch = torch.LongTensor(random.sample(indices, k=proj_batch_size))\n",
    "    outs = proj_layer(target_features[batch].to(device).detach())\n",
    "    loss = -similarity_fn(clip_features[batch].to(device).detach(), outs)\n",
    "        \n",
    "    loss = torch.mean(loss)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    if i%50==0 or i==proj_steps-1:\n",
    "        with torch.no_grad():\n",
    "            val_output = proj_layer(val_target_features.to(device).detach())\n",
    "            val_loss = -similarity_fn(val_clip_features.to(device).detach(), val_output)\n",
    "            val_loss = torch.mean(val_loss)\n",
    "        if i==0:\n",
    "            best_val_loss = val_loss\n",
    "            best_step = i\n",
    "            best_weights = proj_layer.weight.clone()\n",
    "            print(\"Step:{}, Avg train similarity:{:.4f}, Avg val similarity:{:.4f}\".format(best_step, -loss.cpu(),\n",
    "                                                                                               -best_val_loss.cpu()))\n",
    "                \n",
    "        elif val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_step = i\n",
    "            best_weights = proj_layer.weight.clone()\n",
    "        else: #stop if val loss starts increasing\n",
    "            break\n",
    "    opt.zero_grad()\n",
    "        \n",
    "proj_layer.load_state_dict({\"weight\":best_weights})\n",
    "print(\"Best step:{}, Avg val similarity:{:.4f}\".format(best_step, -best_val_loss.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting clear chest x-ray, Iterpretability:0.061\n",
      "Deleting diffuse lesions, Iterpretability:0.148\n",
      "Deleting multifocal lesions, Iterpretability:0.093\n",
      "Deleting no pneumothorax, Iterpretability:0.045\n",
      "Deleting perivascular cuffing, Iterpretability:0.138\n",
      "Deleting the blood vessels, Iterpretability:0.092\n",
      "Deleting the bronchi, Iterpretability:0.042\n"
     ]
    }
   ],
   "source": [
    "#delete concepts that are not interpretable\n",
    "with torch.no_grad():\n",
    "    proj_layer = proj_layer.to(device)\n",
    "    outs = proj_layer(val_target_features.to(device).detach())\n",
    "    sim = similarity_fn(val_clip_features.to(device).detach(), outs)\n",
    "    interpretable = sim > interpretability_cutoff\n",
    "        \n",
    "if prints:\n",
    "    for i, concept in enumerate(concepts):\n",
    "        if sim[i]<=interpretability_cutoff:\n",
    "            print(\"Deleting {}, Iterpretability:{:.3f}\".format(concept, sim[i]))\n",
    "    \n",
    "concepts = [concepts[i] for i in range(len(concepts)) if interpretable[i]]\n",
    "    \n",
    "del clip_features, val_clip_features\n",
    "\n",
    "W_c = proj_layer.weight[interpretable]\n",
    "proj_layer = torch.nn.Linear(in_features=target_features.shape[1], out_features=len(concepts), bias=False)\n",
    "proj_layer.load_state_dict({\"weight\":W_c})\n",
    "\n",
    "train_targets = data_utils.get_targets_only(d_train)\n",
    "val_targets = data_utils.get_targets_only(d_val)\n",
    "    \n",
    "with torch.no_grad():\n",
    "    # feature fusion\n",
    "    train_c = torch.cat([proj_layer(target_features.detach()), target_features.detach()], dim=1)\n",
    "    val_c = torch.cat([proj_layer(val_target_features.detach()), val_target_features.detach()],dim=1)\n",
    "    \n",
    "#     train_c = proj_layer(target_features.detach())\n",
    "#     val_c = proj_layer(val_target_features.detach())\n",
    "    \n",
    "        \n",
    "    train_mean = torch.mean(train_c, dim=0, keepdim=True)\n",
    "    train_std = torch.std(train_c, dim=0, keepdim=True)\n",
    "        \n",
    "    train_c -= train_mean\n",
    "    train_c /= train_std\n",
    "        \n",
    "    train_y = torch.LongTensor(train_targets)\n",
    "    indexed_train_ds = IndexedTensorDataset(train_c, train_y)\n",
    "\n",
    "    val_c -= train_mean\n",
    "    val_c /= train_std\n",
    "        \n",
    "    val_y = torch.LongTensor(val_targets)\n",
    "\n",
    "    val_ds = TensorDataset(val_c,val_y)\n",
    "\n",
    "\n",
    "indexed_train_loader = DataLoader(indexed_train_ds, batch_size=saga_batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=saga_batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2224/10000 [00:11<00:41, 189.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0) lambda 0.0001, loss 0.0015, acc 1.0000 [val acc 0.8069] [test acc -1.0000], sparsity 0.548306148055207 [874/1594], time 11.77230191230774, lr 0.1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Make linear model and zero initialize\n",
    "linear = torch.nn.Linear(train_c.shape[1],len(classes)).to(device)\n",
    "# linear.weight.data.zero_()\n",
    "# linear.bias.data.zero_()\n",
    "    \n",
    "STEP_SIZE = 0.1\n",
    "ALPHA = 0.99\n",
    "metadata = {}\n",
    "metadata['max_reg'] = {}\n",
    "metadata['max_reg']['nongrouped'] = lam\n",
    "\n",
    "# Solve the GLM path\n",
    "output_proj = glm_saga(linear, indexed_train_loader, STEP_SIZE, n_iters, ALPHA, epsilon=1, k=1,\n",
    "                      val_loader=val_loader, do_zero=False, metadata=metadata, n_ex=len(target_features), n_classes = len(classes))\n",
    "W_g = output_proj['path'][0]['weight']\n",
    "b_g = output_proj['path'][0]['bias']\n",
    "\n",
    "    \n",
    "save_name = \"{}/{}_cbm_{}\".format(save_dir, dataset, datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M\"))\n",
    "os.mkdir(save_name)\n",
    "torch.save(train_mean, os.path.join(save_name, \"proj_mean.pt\"))\n",
    "torch.save(train_std, os.path.join(save_name, \"proj_std.pt\"))\n",
    "torch.save(W_c, os.path.join(save_name ,\"W_c.pt\"))\n",
    "torch.save(W_g, os.path.join(save_name, \"W_g.pt\"))\n",
    "torch.save(b_g, os.path.join(save_name, \"b_g.pt\"))\n",
    "    \n",
    "with open(os.path.join(save_name, \"concepts.txt\"), 'w') as f:\n",
    "    f.write(concepts[0])\n",
    "    for concept in concepts[1:]:\n",
    "        f.write('\\n'+concept)\n",
    "dict = {\n",
    "  \"clip_name\": \"{}\".format(clip_name),\n",
    "  \"backbone\": \"{}\".format(backbone),\n",
    "  \"device\": \"{}\".format(device),\n",
    "  \"batch_size\": \"{}\".format(batch_size),\n",
    "  \"saga_batch_size\": \"{}\".format(saga_batch_size),\n",
    "  \"dataset\": \"{}\".format(dataset),\n",
    "  \"concept_set\": \"{}\".format(concept_set),\n",
    "  \"feature_layer\": \"{}\".format(feature_layer),\n",
    "  \"activation_dir\": \"{}\".format(activation_dir),\n",
    "  \"save_dir\": \"{}\".format(save_dir),\n",
    "  \"clip_cutoff\": \"{}\".format(clip_cutoff),\n",
    "  \"proj_steps\": \"{}\".format(proj_steps),\n",
    "  \"interpretability_cutoff\": \"{}\".format(interpretability_cutoff),\n",
    "  \"lam\": '{}'.format(lam),\n",
    "  \"n_iters\": \"{}\".format(n_iters)\n",
    "}\n",
    "with open(os.path.join(save_name, \"args.txt\"), 'w') as f:\n",
    "    json.dump(dict, f, indent=2)\n",
    "    \n",
    "with open(os.path.join(save_name, \"metrics.txt\"), 'w') as f:\n",
    "    out_dict = {}\n",
    "    for key in ('lam', 'lr', 'alpha', 'time'):\n",
    "        out_dict[key] = float(output_proj['path'][0][key])\n",
    "    out_dict['metrics'] = output_proj['path'][0]['metrics']\n",
    "    nnz = (W_g.abs() > 1e-5).sum().item()\n",
    "    total = W_g.numel()\n",
    "    out_dict['sparsity'] = {\"Non-zero weights\":nnz, \"Total weights\":total, \"Percentage non-zero\":nnz/total}\n",
    "    json.dump(out_dict, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CBM",
   "language": "python",
   "name": "cbm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
